{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix calculus on Deep learning\n",
    "\n",
    "For example, the activation of a single computation unit in a neural network is typically calculated using the dot product (from linear algebra) of an edge weight vector w with an input vector x plus a scalar bias (threshold): \n",
    "![](https://explained.ai/matrix-calculus/images/eqn-EEDCFA4252D0992243A283CE0EB777A6-depth003.31.svg)\n",
    "Function  is called the unit's affine function and is followed by a rectified linear unit, which clips negative values to zero: . Such a computational unit is sometimes referred to as an “artificial neuron” and looks like:\n",
    "<div>\n",
    "<img src=\"https://explained.ai/matrix-calculus/images/neuron.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "Neural networks consist of many of these units, organized into multiple collections of neurons called layers. The activation of one layer's units become the input to the next layer's units. The activation of the unit or units in the final layer is called the network output.\n",
    "\n",
    "Training this neuron means choosing weights w and bias b so that we get the desired output for all N inputs x. To do that, we minimize a loss function that compares the network's final  with the  (desired output of x) for all input x vectors. To minimize the loss, we use some variation on gradient descent, such as plain stochastic gradient descent (SGD), SGD with momentum, or Adam. All of those require the partial derivative (the gradient) of  with respect to the model parameters w and b. Our goal is to gradually tweak w and b so that the overall loss function keeps getting smaller across all x inputs.\n",
    "\n",
    "But this is just one neuron, and neural networks must train the weights and biases of all neurons in all layers simultaneously. Because there are multiple inputs and (potentially) multiple network outputs, we really need general rules for the derivative of a function with respect to a vector and even rules for the derivative of a vector-valued function with respect to a vector.This field is known as **matrix calculus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar derivative rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1500/1*ZR50K2cDpl1um4S-aOeWQw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "![](https://explained.ai/matrix-calculus/images/blkeqn-A1EC7F214318E08949CC8BFCED138D94.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector calculus\n",
    "\n",
    "Neural network layers are not single functions of a single parameter, f(x). So, let’s move on to\n",
    "functions of multiple parameters such as f(x, y). \n",
    "\n",
    "For example, what is the derivative of xy (i.e.,\n",
    "the multiplication of x and y)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute derivatives with respect to one variable (parameter) at a time, giving us two different **partial derivatives** for this two parameter function (one for x and one for y). \n",
    "\n",
    "Instead of using operator d/dx , the partial derivative operator is ∂/∂x\n",
    "\n",
    "So, ∂/∂x(xy) and ∂/∂y(xy) are the partial derivatives of xy. often, these are just called the partials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative with respect to x is just the usual scalar derivative, simply treating any other\n",
    "variable in the equation as a constant.\n",
    "\n",
    "Consider a function ![](https://explained.ai/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg)\n",
    "\n",
    "The partial derivative with respect to x is ![](https://explained.ai/matrix-calculus/images/eqn-D981BA4BD14AC44C43A4E4E0EC750B4A-depth004.67.svg)\n",
    "\n",
    "The partial derivative with respect to y treats x like a constant: ![](https://explained.ai/matrix-calculus/images/eqn-55A3A400FAD3326FEF1BB9DDD2658383-depth006.34.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let’s organize them into a horizontal vector. We call this vector the gradient of f(x, y) and write it as:\n",
    "\n",
    "![](https://explained.ai/matrix-calculus/images/blkeqn-0C95BB61B2BFFB0C2A95A9DC5D8AF44E.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives with vectors\n",
    "\n",
    "#### Vector-by-scalar\n",
    "The derivative of a vector \n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/598107eef2ea088f6bf06ffced12d581ca330e43)\n",
    "by a scalar x is written (in numerator layout notation) as\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/94d7a5da91350e1178afb955a08a6317dc8de783)\n",
    "\n",
    "#### Scalar-by-vector\n",
    "The derivative of a scalar y by a vector ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a73d2b3df42b37c7d42eab9dd34070aba9feff1c)\n",
    "is written (in numerator layout notation) as\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/b02ac6c5072b46943d3406a440659f2b2d1e54cc)\n",
    "\n",
    "#### Vector-by-vector\n",
    "The derivative of a vector function ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/598107eef2ea088f6bf06ffced12d581ca330e43)\n",
    "with respect to an input vector,\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a73d2b3df42b37c7d42eab9dd34070aba9feff1c)\n",
    "is written (in numerator layout notation) as\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/43b8c31f61f8b2bbe6f0df4d62012d8ba86ba420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
